---
title: "Functions for S3 buckets"
format: html
editor: 
  markdown: 
    wrap: 72
---

## `read_from_s3()`

`read_from_s3()` loads spatial data directly from an S3 bucket and returns it as either an `sf` object (for vector data) or a `SpatRaster` (for raster data). The function detects the file format automatically from the file extension unless you specify it manually.  

The function supports:
- **GeoParquet (`.parquet`)** — read directly via Arrow + sfarrow  
- **GeoPackage (`.gpkg`)** — downloaded temporarily, then read with `sf`  
- **GeoTIFF (`.tif`)** — downloaded temporarily, then read with `terra`  

Internally, the function handles all S3 communication, temporary files, and format-specific reading. This means you can load S3-based spatial data with a single line of code, without worrying about URLs, credentials, or file formats.

This function is useful when:
- you store datasets in buckets instead of local directories  
- you work with mixed formats (e.g., raster + vector)  
- you need a consistent interface for reading data from internal S3  
- you prefer lightweight, single-file spatial formats such as Parquet, GPKG, and GeoTIFF  


## `write_to_s3()`

`write_to_s3()` uploads spatial data (vector or raster) to an S3 bucket.  
It handles both the *conversion* to a single-file format and the *upload* to the bucket.

Supported output formats include:
- **`.gpkg` (GeoPackage)** — supports multiple internal layers  
- **`.parquet` (GeoParquet)** — lightweight and highly portable  
- **`.tif` (GeoTIFF)** — for raster data

Not supported:
- **shapefiles (`.shp`)**, because they require multiple companion files  
- **Esri File Geodatabases (`.gdb`)**, because they are folder-based formats  

Vectors (`sf`) are written using either `st_write()` or `sfarrow::st_write_parquet()`, and rasters (`SpatRaster`) are written with `terra::writeRaster()`.  
The function then uploads the resulting file to S3 using `aws.s3::put_object()`.

This function is especially helpful for:
- storing analysis-ready datasets in shared internal buckets  
- pipeline automation where outputs need to be uploaded as part of processing  
- converting spatial formats to stable, single-file S3-friendly versions  
- ensuring that exported data is consistently written with a reproducible workflow  


## `list_from_s3()`

`list_from_s3()` lists the files stored in an S3 bucket, with optional filtering by prefix (a folder-like path) or by file extension.  
It returns either:
- a **vector of keys** (file paths), or  
- a **data frame with metadata** (size, last modified time)

You can use `prefix` to list only the contents of a specific subfolder, such as:

```
prefix = "grunnkart/"
```

You can also limit the results to a specific file type, such as:

```
file_type = ".parquet"
```

This function is a practical way to:
- explore datasets stored in S3 buckets  
- script reproducible workflows that need to know which files exist  
- automatically select resources or versions based on bucket contents  
- check for expected input/output files in bucket-based pipelines  

Together, these three functions provide a streamlined way to browse, read, and write spatial datasets stored inside NINA’s internal S3 infrastructure.
